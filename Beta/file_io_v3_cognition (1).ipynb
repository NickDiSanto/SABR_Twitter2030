{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0f879e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0XrBxAZH8Jh",
    "outputId": "00c8a102-8f53-4118-a0cd-14e709bd1b2b"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "241d9773",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52a8ca18-a34b-4cd1-ae8b-7c8c173bc937",
    "outputId": "87c7594c-a5a5-45ad-b4f4-565d8dc07c1f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Fin 4405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Folder Loop\n",
    "import os\n",
    "\n",
    "# Folder of .gz tweet files\n",
    "path_of_the_directory = \"/media/SGBackupE2/Working/04_Research/02_Projects/Twitter2030/02_Data/Kappa\"\n",
    "\n",
    "file_extension = '.gz' # File extension for unzip\n",
    "file_list = [] # Create a list of file names.\n",
    "count = 0 # Counter\n",
    "\n",
    "# Build list of files to uncompress\n",
    "for filename in os.listdir(path_of_the_directory):\n",
    "    f = os.path.join(path_of_the_directory, filename)\n",
    "    if os.path.isfile(f):\n",
    "        if f.endswith(file_extension):\n",
    "            #print(f)\n",
    "            file_list.append(f)\n",
    "            count += 1\n",
    "\n",
    "print('El Fin', count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "634d395d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4e3fddb-0659-45d5-bcc1-7faf6da5a32a",
    "outputId": "b102f27f-57b8-40e1-dba0-3cca9dec67f1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get Number of Files in List\n",
    "\n",
    "print(len(file_list))\n",
    "#or x in file_list:\n",
    "#print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b593f90",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c2d959c-222d-4665-93f1-754274369a91",
    "outputId": "d34a13cf-47b9-4b6d-a289-325bed572c4a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Gzip Uncompress\n",
    "import gzip\n",
    "\n",
    "# Function to uncompress a .gz file and then save it\n",
    "def gunzip(source_filepath, dest_filepath, block_size=65536):\n",
    "    with gzip.open(source_filepath, 'rb') as s_file, \\\n",
    "            open(dest_filepath, 'wb') as d_file:\n",
    "        while True:\n",
    "            block = s_file.read(block_size)\n",
    "            if not block:\n",
    "                break\n",
    "            else:\n",
    "                d_file.write(block)\n",
    "\n",
    "# gunzip function call to unzip each .gz file and save it\n",
    "for file in file_list:\n",
    "    source_file = file\n",
    "    dest_file = file.rstrip('.gz')\n",
    "    dest_file = dest_file + '.json'\n",
    "    #print(source_file, dest_file)\n",
    "    gunzip(source_file, dest_file)\n",
    "\n",
    "print('El Fin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea6902d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "195d6c39-57df-41e5-954e-eff6d17dcda9",
    "outputId": "f3b1cb6f-7271-4740-bddf-3edf0561a9a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Fin 4405\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Folder Loop for Uncompressed JSON files\n",
    "import os\n",
    "\n",
    "# Folder of .gz tweet files\n",
    "path_of_the_directory_json = \"/media/SGBackupE2/Working/04_Research/02_Projects/Twitter2030/02_Data/Kappa\"\n",
    "\n",
    "file_extension_json = '.json' # File extension for unzip\n",
    "file_list_json = [] # Create a list of file names.\n",
    "count_json = 0 # Counter\n",
    "\n",
    "# Build list of json files to read\n",
    "for filename_json in os.listdir(path_of_the_directory_json):\n",
    "    json_file = os.path.join(path_of_the_directory_json, filename_json)\n",
    "\n",
    "    if os.path.isfile(json_file):\n",
    "        if json_file.endswith(file_extension_json):\n",
    "            #print(json_file)\n",
    "            file_list_json.append(json_file)\n",
    "            count_json += 1\n",
    "\n",
    "print('El Fin', count_json)\n",
    "\n",
    "#for json_file_read in file_list_json:\n",
    "#    print(json_file_read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf0bd53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41f74675-1cfb-4eea-93af-9020edd1d105",
    "outputId": "3ba1d3f7-f053-4893-e157-4f6530d9e45e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dest_filepath = file_list_json[0]\n",
    "\n",
    "input_file = open(dest_filepath, 'r')\n",
    "\n",
    "tweet = input_file.readline()\n",
    "count = 0\n",
    "while(tweet != ''):\n",
    "    print(tweet)\n",
    "    tweet = input_file.readline()\n",
    "    count += 1\n",
    "\n",
    "input_file.close()\n",
    "print(\"total count is:\", count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfde5b",
   "metadata": {
    "id": "17b86372-ba64-47c5-a5d3-53683bb8107c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://pypi.org/project/tweet-parser/\n",
    "from tweet_parser.tweet import Tweet\n",
    "from tweet_parser.tweet_parser_errors import NotATweetError\n",
    "import fileinput\n",
    "import json\n",
    "\n",
    "for line in fileinput.FileInput(file_list_json[1]):\n",
    "    try:\n",
    "        tweet_dict = json.loads(line)\n",
    "        # print(tweet_dict)\n",
    "        tweet = Tweet(tweet_dict)\n",
    "        # for ky_val in tweet_dict.keys():\n",
    "        #     print(len(ky_val))\n",
    "        # print()\n",
    "    except (json.JSONDecodeError, NotATweetError):\n",
    "        pass\n",
    "    print(tweet.created_at_string, \"\\n\", tweet.all_text, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dac66bf1",
   "metadata": {
    "id": "ef5369b9-d3d5-4703-9b01-70bb1dc23ebd"
   },
   "outputs": [],
   "source": [
    "# https://pypi.org/project/tweet-parser/\n",
    "from tweet_parser.tweet import Tweet\n",
    "from tweet_parser.tweet_parser_errors import NotATweetError\n",
    "import fileinput\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# White Sox Game Dates\n",
    "sox_game_dates = set([\n",
    "    '2019-04-05', '2019-04-06', '2019-04-07', '2019-04-08', '2019-04-09', '2019-04-10',\n",
    "    '2019-04-15', '2019-04-16', '2019-04-17', '2019-04-26', '2019-04-28', '2019-04-29',\n",
    "    '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-04', '2019-05-05', '2019-05-13',\n",
    "    '2019-05-14', '2019-05-16', '2019-05-17', '2019-05-18', '2019-05-19', '2019-05-27',\n",
    "    '2019-05-28', '2019-05-29', '2019-05-30', '2019-05-31', '2019-06-01', '2019-06-02',\n",
    "    '2019-06-10', '2019-06-11', '2019-06-13', '2019-06-14', '2019-06-15', '2019-06-16',\n",
    "    '2019-06-28', '2019-06-29', '2019-06-30', '2019-07-03', '2019-07-04', '2019-07-06',\n",
    "    '2019-07-07', '2019-07-22', '2019-07-23', '2019-07-24', '2019-07-25', '2019-07-26',\n",
    "    '2019-07-27', '2019-07-28', '2019-07-30', '2019-07-31', '2019-08-01', '2019-08-09',\n",
    "    '2019-08-10', '2019-08-11', '2019-08-13', '2019-08-14', '2019-08-22', '2019-08-23',\n",
    "    '2019-08-24', '2019-08-25', '2019-08-27', '2019-08-28', '2019-08-29', '2019-09-06',\n",
    "    '2019-09-07', '2019-09-08', '2019-09-10', '2019-09-11', '2019-09-12', '2019-09-24',\n",
    "    '2019-09-25', '2019-09-26', '2019-09-28', '2019-09-29'])\n",
    "\n",
    "# Chicago Cubs Game Dates\n",
    "#cubs_game_dates = set([\n",
    "#    '2019-04-08', '2019-04-10', '2019-04-11', '2019-04-12', '2019-04-13', '2019-04-19',\n",
    "#    '2019-04-20', '2019-04-21', '2019-04-23', '2019-04-24', '2019-04-25', '2019-05-03',\n",
    "#    '2019-05-04', '2019-05-05', '2019-05-06', '2019-05-07', '2019-05-08', '2019-05-09',\n",
    "#    '2019-05-10', '2019-05-11', '2019-05-12', '2019-05-20', '2019-05-21', '2019-05-22',\n",
    "#    '2019-05-23', '2019-05-24', '2019-05-25', '2019-05-26', '2019-06-03', '2019-06-04',\n",
    "#    '2019-06-05', '2019-06-06', '2019-06-07', '2019-06-08', '2019-06-09', '2019-06-18',\n",
    "#    '2019-06-19', '2019-06-20', '2019-06-21', '2019-06-22', '2019-06-23', '2019-06-24',\n",
    "#    '2019-06-25', '2019-06-26', '2019-06-27', '2019-07-12', '2019-07-13', '2019-07-14',\n",
    "#    '2019-07-15', '2019-07-16', '2019-07-17', '2019-07-19', '2019-07-20', '2019-07-21',\n",
    "#    '2019-08-02', '2019-08-03', '2019-08-04', '2019-08-05', '2019-08-06', '2019-08-07',\n",
    "#    '2019-08-20', '2019-08-21', '2019-08-22', '2019-08-23', '2019-08-24', '2019-08-25',\n",
    "#    '2019-08-30', '2019-08-31', '2019-09-01', '2019-09-02', '2019-09-03', '2019-09-13',\n",
    "#    '2019-09-14', '2019-09-15', '2019-09-16', '2019-09-17', '2019-09-18', '2019-09-19',\n",
    "#    '2019-09-20', '2019-09-21', '2019-09-22'])\n",
    "\n",
    "super_list = [['NOUN', 'VERB', 'ADV'], ['NOUN', 'VERB', 'ADP'],\n",
    "              ['NOUN', 'VERB', 'ADV', 'ADJ'], ['NOUN', 'VERB', 'NOUN', 'ADP'],\n",
    "              ['NOUN', 'VERB', 'ADJ'], ['NOUN', 'VERB', 'ADJ', 'NOUN']]\n",
    "\n",
    "def clean_tweets(tweet):\n",
    "\t# remove stock market tickers like $GE\n",
    "\ttweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "\n",
    "\t# remove old style retweet text \"RT\"\n",
    "\ttweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "\t# remove hyperlinks\n",
    "\ttweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\t\n",
    "\t# remove hashtags\n",
    "\t# only removing the hash # sign from the word\n",
    "\ttweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "\treturn tweet\n",
    "\n",
    "# Dictionaries: Key - Date    Value - Number of tweets of that sentiment\n",
    "yes_cognition_dict = {}\n",
    "no_cognition_dict = {}\n",
    "\n",
    "# Iterates through all JSON files\n",
    "for line in fileinput.FileInput(file_list_json):\n",
    "    try:\n",
    "        tweet_dict = json.loads(line)\n",
    "\n",
    "        tweet = Tweet(tweet_dict)\n",
    "\n",
    "        text = clean_tweets(tweet.all_text)\n",
    "\n",
    "        # Extracts date and verifies its a day the Sox play\n",
    "        date = tweet.created_at_string[0:10]\n",
    "\n",
    "        if date in sox_game_dates:\n",
    "            tokens = nltk.word_tokenize(text) # Create a list of tokens\n",
    "            tokens_tagged = nltk.pos_tag(tokens, tagset='universal')\n",
    "\n",
    "            pos_tokens = ''\n",
    "\n",
    "            # Loop the dict of tokens\n",
    "            for token in tokens_tagged:\n",
    "                pos_tokens += token[1] + ' '\n",
    "\n",
    "            sentence = str(pos_tokens)#'Whoever is happy will make others happy too'\n",
    "            trigrams = ngrams(sentence.split(), 3)\n",
    "            quadgrams = ngrams(sentence.split(), 4)\n",
    "\n",
    "            trigram_list = []\n",
    "            for item in trigrams:\n",
    "                trigram_list.append(item[0])\n",
    "                trigram_list.append(item[1])\n",
    "                trigram_list.append(item[2])\n",
    "\n",
    "            quadgram_list = []\n",
    "            for item in quadgrams:\n",
    "                quadgram_list.append(item[0])\n",
    "                quadgram_list.append(item[1])\n",
    "                quadgram_list.append(item[2])\n",
    "                quadgram_list.append(item[3])\n",
    "\n",
    "\n",
    "            if trigram_list in super_list or quadgram_list in super_list:\n",
    "                if date in yes_cognition_dict.keys():\n",
    "                    yes_cognition_dict[date] += 1\n",
    "                else:\n",
    "                    yes_cognition_dict[date] = 1\n",
    "            else:\n",
    "                if date in no_cognition_dict.keys():\n",
    "                    no_cognition_dict[date] += 1\n",
    "                else:\n",
    "                    no_cognition_dict[date] = 1\n",
    "\n",
    "    except (json.JSONDecodeError, NotATweetError):\n",
    "        pass\n",
    "\n",
    "# Writes to text files\n",
    "with open(\"sox_cognition_yes.txt\", 'w') as new_file: \n",
    "    for key, value in yes_cognition_dict.items(): \n",
    "        new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "with open(\"sox_cognition_no.txt\", 'w') as new_file: \n",
    "    for key, value in no_cognition_dict.items(): \n",
    "        new_file.write('%s : %d\\n' % (key, value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9027402f-f5d1-4d34-a797-bf6c45638086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "file_io_v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
