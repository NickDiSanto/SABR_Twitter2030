{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a18fe5b-a6cf-46df-a8ee-e6d42164a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweet_parser.tweet import Tweet\n",
    "from tweet_parser.tweet_parser_errors import NotATweetError, UnexpectedFormatError\n",
    "import fileinput\n",
    "import json\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import geopy.distance\n",
    "import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "# nltk.download('universal_tagset')\n",
    "from nltk.util import ngrams\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from google.cloud import storage\n",
    "\n",
    "# Create bucket client and authenticate via service account access key\n",
    "storage_client = storage.Client.from_service_account_json('/srv/Twitter2030/Data/acts-cloud-5-tljh-8e85a08eab1d-sakey.json')\n",
    "\n",
    "BUCKET_NAME = 'allteamsmlb' # Do not change\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "print('El Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582266c5-4af5-41ea-af82-ba1f575a2366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Fin\n"
     ]
    }
   ],
   "source": [
    "team_names = [\n",
    "    '02Angels', '03Athletics', '04Mariners', '05Astros', '06Rangers', '07Rockies', '08Padres', '09Dodgers', '10Giants', '11Diamondbacks',\n",
    "    '12Cardinals', '13Reds', '14Cubs', '15Pirates', '16Brewers', '17Braves', '18Mets', '19Phillies', '20Marlins', '21Nationals',\n",
    "    '22Indians', '23Twins', '24Royals', '25Tigers', '26WhiteSox', '27Yankees', '28Rays', '29RedSox', '30Orioles', '31BlueJays'\n",
    "]\n",
    "\n",
    "team_abbrevs = [\n",
    "    'ANA', 'OAK', 'SEA', 'HOU', 'TEX', 'COL', 'SDP', 'LAD', 'SFG', 'AZ', 'STL', 'CIN', 'CHC', 'PIT', 'MIL',\n",
    "    'ATL', 'NYM', 'PHI', 'FLA', 'WSN', 'CLE', 'MIN', 'KCR', 'DET', 'CHW', 'NYY', 'TBD', 'BOS', 'BAL', 'TOR'\n",
    "]\n",
    "\n",
    "# TODO: Check for negative longitudes in the tweets\n",
    "team_lats = {\n",
    "    'ANA': 33.800031, 'OAK': 37.751614, 'SEA': 47.591217, 'HOU': 29.757224, 'TEX': 32.751207,\n",
    "    'COL': 39.755878, 'SDP': 32.707206, 'LAD': 34.07358, 'SFG': 37.778361, 'AZ': 33.445498,\n",
    "    'STL': 38.62248, 'CIN': 39.097392, 'CHC': 41.948036, 'PIT': 40.446993, 'MIL': 43.02834,\n",
    "    'ATL': 33.890961, 'NYM': 40.756743, 'PHI': 39.905763, 'FLA': 25.778194, 'WSN': 38.872705,\n",
    "    'CLE': 41.496183, 'MIN': 44.981703, 'KCR': 39.051355, 'DET': 42.339308, 'CHW': 41.830087,\n",
    "    'NYY': 40.829519, 'TBD': 27.768214, 'BOS': 42.346361, 'BAL': 39.283658, 'TOR': 43.641684\n",
    "}\n",
    "\n",
    "team_longs = {\n",
    "    'ANA': -117.883017, 'OAK': -122.200574, 'SEA': -122.332721, 'HOU': -95.35521, 'TEX': -97.082635,\n",
    "    'COL': -104.994192, 'SDP': -117.15706, 'LAD': -118.240147, 'SFG': -122.389712, 'AZ': -112.066694,\n",
    "    'STL': -90.193205, 'CIN': -84.506852, 'CHC': -87.65569, 'PIT': -80.005987, 'MIL': -87.971451,\n",
    "    'ATL': -84.467772, 'NYM': -73.845994, 'PHI': -75.166574, 'FLA': -80.219668, 'WSN': -77.007632,\n",
    "    'CLE': -81.685699, 'MIN': -93.278072, 'KCR': -94.480666, 'DET': -83.048876, 'CHW': -87.63405,\n",
    "    'NYY': -73.926739, 'TBD': -82.653295, 'BOS': -71.097631, 'BAL': -76.621801, 'TOR': -79.389235\n",
    "}\n",
    "\n",
    "super_list_old = [\n",
    "    ['NOUN', 'VERB', 'ADV'], ['NOUN', 'VERB', 'ADP'], ['NOUN', 'VERB', 'ADV', 'ADJ'],\n",
    "    ['NOUN', 'VERB', 'NOUN', 'ADP'], ['NOUN', 'VERB', 'ADJ'], ['NOUN', 'VERB', 'ADJ', 'NOUN']\n",
    "]\n",
    "\n",
    "# TODO: Get new super_list\n",
    "super_list_new = []\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy_old = [\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':c)', ':>', '=]', '=)', ':}', ':^)', ':-D',\n",
    "    ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', ':-))', \":'-)\",\n",
    "    \":')\", ':^*', '>:P', ':-P', ':P', ':-p', ':p', '=p', ':-b', ':b', '>:)',\n",
    "    '>;)', '>:-)', '<3', '☺️', '😚', '😙', '😋', '😛', '😜', '😝', '🤗', '😏',\n",
    "    '😌', '🥳', '😎', '😺', '😸', '😹', '😻', '😼', '😽', '👍', '👏', '🙌',\n",
    "    '😀', '😃', '😄', '😁', '😆', '😅', '🤣', '😂', '🙂', '😉', '😊', '😇',\n",
    "    '🥰', '😍', '🤩', '😘', '🎉', '🎊', '💘', '💝', '💖', '💗', '💓', '💞',\n",
    "    '💕', '💟', '❣️', '❤️‍🔥', '❤️', '🧡', '💛', '💚', '💙', '💜', '🤎', '🖤',\n",
    "    '🤍', '💯'\n",
    "    ]\n",
    "\n",
    "emoticons_happy_new = [\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':c)', '=]', '=)', ':^)', ':D', 'xD', 'XD',\n",
    "    '=-D', '=D', ':-))', \":'-)\", \":')\", '>:)', '>;)', '>:-)', '<3', '☺️', '😚',\n",
    "    '😙', '😋', '😛', '😜', '😝', '🤗', '😏', '😌', '🥳', '😎', '😺', '😸', '😹', '😻',\n",
    "    '😼', '😽', '👍', '👏', '🙌', '😀', '😃', '😄', '😁', '😆', '😅', '🤣', '😂', '🙂',\n",
    "    '😉', '😊', '😇', '🥰', '😍', '🤩', '😘', '🎉', '🎊', '💘', '💝', '💖', '💗', '💓',\n",
    "    '💞', '💕', '💟', '❣️', '❤️‍🔥', '❤️', '🧡', '💛', '💚', '💙', '💜', '🤎',\n",
    "    '🖤', '🤍', '💯'\n",
    "    ]\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad_old = [\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', '>:\\\\', ';(', '😒', '😔', '🤢', '🤮', '🤧', '🥴', '😕', '😟', '🙁',\n",
    "    '☹️', '😦', '😧', '😨', '😰', '😥', '😢', '😭', '😱', '😖', '😣', '😞',\n",
    "    '😓', '😩', '😫', '😤', '😡', '😠', '🤬', '👿', '😿', '😾', '🖕', '👎',\n",
    "    '🙍', '🙍‍♂️', '🙍‍♀️', '🙎', '🙎‍♂️', '🙎‍♀️', '🤦', '🤦‍♂️', '🤦‍♀️', '💔', '💢'\n",
    "    ]\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad_new = [\n",
    "    ':-/', '>:/', '>:[', ':-(', ':[', ':-||', ':-[', ':-<', '=\\\\', '=/', '>:(',\n",
    "    ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', '>:\\\\', ';(', '😒', '😔', '🤢',\n",
    "    '🤮', '🤧', '🥴', '😕', '😟', '🙁', '☹️', '😦', '😧', '😨', '😰', '😥', '😢',\n",
    "    '😭', '😱', '😖', '😣', '😞', '😓', '😩', '😫', '😤', '😡', '😠', '🤬', '👿', '😿',\n",
    "    '😾', '🖕', '👎', '🙍', '🙍‍♂️', '🙍‍♀️', '🙎', '🙎‍♂️', '🙎‍♀️', '🤦', '🤦‍♂️',\n",
    "    '🤦‍♀️', '💔', '💢'\n",
    "    ]\n",
    "\n",
    "print('El Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7d8506-bce8-4eea-9088-28be2c107c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El Fin\n"
     ]
    }
   ],
   "source": [
    "def clean_tweet(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "\n",
    "    # remove old style retweet text 'RT'\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "\n",
    "intervals = (\n",
    "    ('weeks', 604800),  # 60 * 60 * 24 * 7\n",
    "    ('days', 86400),  # 60 * 60 * 24\n",
    "    ('hours', 3600),  # 60 * 60\n",
    "    ('minutes', 60),\n",
    "    ('seconds', 1),\n",
    ")\n",
    "\n",
    "\n",
    "def display_time(seconds, granularity=2):\n",
    "    result = []\n",
    "\n",
    "    for name, count in intervals:\n",
    "        value = seconds // count\n",
    "        if value:\n",
    "            seconds -= value * count\n",
    "            if value == 1:\n",
    "                name = name.rstrip('s')\n",
    "            result.append(\"{} {}\".format(round(value), name))\n",
    "    return ', '.join(result[:granularity])\n",
    "\n",
    "\n",
    "def increment_dict(key, dictionary):\n",
    "    if key in dictionary.keys():\n",
    "        dictionary[key] += 1\n",
    "    else:\n",
    "        dictionary[key] = 1\n",
    "\n",
    "\n",
    "def write_dict_to_file(file_name, dictionary):\n",
    "    with open(file_name, 'w') as new_file:\n",
    "        for key, value in dictionary.items():\n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "print('El Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89336f3-1b71-44dd-a73a-4f053fa44b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start runtime\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "# ALL TEAMS DICTS\n",
    "ALL_TEAMS_total_bow = {}\n",
    "ALL_TEAMS_total_tweets = {}\n",
    "\n",
    "\n",
    "for team_name in team_names:\n",
    "\n",
    "    team_abbrev = team_abbrevs[team_names.index(team_name)]\n",
    "\n",
    "    dates = []\n",
    "\n",
    "    for i in range(16, 20):\n",
    "        # Atlanta 2016 is in a different stadium. Skip.\n",
    "        if team_abbrev == 'ATL' and i == 16:\n",
    "            continue\n",
    "        with open(('Data/Team_WL/' + str(team_abbrev) + str(i) + '.csv'), newline='', encoding='utf-8-sig') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                date = row[0]\n",
    "                if (date.startswith('d')):\n",
    "                    continue\n",
    "                if not date.startswith('1'):\n",
    "                    date = '0' + date\n",
    "                if not date[5] == '/':\n",
    "                    date = date[:3] + '0' + date[3:]\n",
    "                date = date[6:] + '-' + date[:2] + '-' + date[3:-5]\n",
    "                dates.append(date)\n",
    "\n",
    "\n",
    "    # Average distances between tweets and stadium\n",
    "    COM_average_distance = 0\n",
    "    IP_average_distance = 0\n",
    "    NP_average_distance = 0\n",
    "\n",
    "    # COMBINED DICTS\n",
    "\n",
    "    COM_total_bow = {}\n",
    "    COM_total_tweets = {}\n",
    "    COM_tweets_per_hour = {}\n",
    "\n",
    "    # DICTS FOR TEAM IS PLAYING\n",
    "\n",
    "    IP_total_bow = {}\n",
    "\n",
    "    # Dictionaries: Key - Date/Time    Value - Number of tweets\n",
    "    IP_total_tweets = {}\n",
    "    IP_tweets_per_hour = {}\n",
    "\n",
    "    IP_neg_polarity = {}\n",
    "    IP_neu_polarity = {}\n",
    "    IP_pos_polarity = {}\n",
    "    IP_com_polarity = {}\n",
    "\n",
    "    IP_yes_cognition_old = {}\n",
    "    IP_no_cognition_old = {}\n",
    "\n",
    "    IP_yes_cognition_new = {}\n",
    "    IP_no_cognition_new = {}\n",
    "\n",
    "    IP_happy_sentiment_old = {}\n",
    "    IP_sad_sentiment_old = {}\n",
    "    IP_general_sentiment_old = {}\n",
    "\n",
    "    IP_happy_sentiment_new = {}\n",
    "    IP_sad_sentiment_new = {}\n",
    "    IP_general_sentiment_new = {}\n",
    "\n",
    "\n",
    "    # DICTS FOR TEAM IS NOT PLAYING\n",
    "\n",
    "    NP_total_bow = {}\n",
    "\n",
    "    # Dictionaries: Key - Date/Time    Value - Number of tweets\n",
    "    NP_total_tweets = {}\n",
    "    NP_tweets_per_hour = {}\n",
    "\n",
    "    NP_neg_polarity = {}\n",
    "    NP_neu_polarity = {}\n",
    "    NP_pos_polarity = {}\n",
    "    NP_com_polarity = {}\n",
    "\n",
    "    NP_yes_cognition_old = {}\n",
    "    NP_no_cognition_old = {}\n",
    "\n",
    "    NP_yes_cognition_new = {}\n",
    "    NP_no_cognition_new = {}\n",
    "\n",
    "    NP_happy_sentiment_old = {}\n",
    "    NP_sad_sentiment_old = {}\n",
    "    NP_general_sentiment_old = {}\n",
    "\n",
    "    NP_happy_sentiment_new = {}\n",
    "    NP_sad_sentiment_new = {}\n",
    "    NP_general_sentiment_new = {}\n",
    "\n",
    "    object_generator = storage_client.list_blobs(BUCKET_NAME, prefix=team_name, delimiter=None)\n",
    "\n",
    "    for status_log_bucket_name in object_generator:\n",
    "\n",
    "        # TODO: Delete me\n",
    "        bucket_time = time.time()\n",
    "        if bucket_time - t0 > 10:\n",
    "            break\n",
    "\n",
    "        client2 = storage.Client()\n",
    "        BUCKET_NAME_TWO = client2.bucket(BUCKET_NAME)\n",
    "        blob = BUCKET_NAME_TWO.blob(status_log_bucket_name.name)\n",
    "\n",
    "        with blob.open('rt') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    raw_tweet = json.loads(line)\n",
    "\n",
    "                    # TODO: Delete me\n",
    "                    start_parse_time = time.time()\n",
    "                    print('\\nACCESS TIME  - ', start_parse_time - bucket_time, '\\n')\n",
    "\n",
    "                    tweet = Tweet(raw_tweet)\n",
    "\n",
    "                    text = clean_tweet(tweet.all_text)\n",
    "                    \n",
    "                    print('Text:', text, '\\n\\n')\n",
    "\n",
    "                    # Extracts time, date, and language\n",
    "                    date = tweet.created_at_string[0:10]\n",
    "                    print('Date:', date, '\\n')\n",
    "                    hour = int(tweet.created_at_string[11:13])\n",
    "                    print('Hour:', hour, '\\n')\n",
    "                    lang = tweet.lang\n",
    "                    print('Lang:', lang, '\\n')\n",
    "\n",
    "                    # Increment combined total number of tweets\n",
    "                    increment_dict(date, COM_total_tweets)\n",
    "                    print('COM_total_tweets:', COM_total_tweets, '\\n')\n",
    "                    increment_dict(date, ALL_TEAMS_total_tweets)\n",
    "                    print('ALL_TEAMS_total_tweets:', ALL_TEAMS_total_tweets, '\\n')\n",
    "                    increment_dict(hour, COM_tweets_per_hour)\n",
    "                    print('COM_tweets_per_hour:', COM_tweets_per_hour, '\\n')\n",
    "\n",
    "                    # Increment combined total BoW\n",
    "                    words = text.split()\n",
    "                    for word in words:\n",
    "                        increment_dict(word, COM_total_bow)\n",
    "                        increment_dict(word, ALL_TEAMS_total_bow)\n",
    "                        \n",
    "                    print('COM_total_bow:', COM_total_bow, '\\n')\n",
    "                    print('ALL_TEAMS_total_bow:', ALL_TEAMS_total_bow, '\\n')\n",
    "                    \n",
    "\n",
    "                    # Gets coordinates from tweet\n",
    "                    coordinates = tweet.geo_coordinates\n",
    "                    if coordinates is not None and coordinates['latitude'] is not None and coordinates['longitute'] is not None:\n",
    "                        tweet_lat = coordinates['latitude']\n",
    "                        tweet_long = coordinates['longitute']\n",
    "\n",
    "                        tweet_coords = (tweet_lat, tweet_long)\n",
    "\n",
    "                        team_lat = team_lats[team_abbrev]\n",
    "                        team_long = team_longs[team_abbrev]\n",
    "\n",
    "                        team_coords = (team_lat, team_long)\n",
    "\n",
    "                        # Distance between the tweet and stadium (meters)\n",
    "                        distance = geopy.distance.geodesic(tweet_coords, team_coords).km * 1000\n",
    "                        print('Tweet coords:', tweet_coords, '\\n')\n",
    "                        print('Team coords:', team_coords, '\\n')\n",
    "                        print('Distance:', distance, '\\n')\n",
    "\n",
    "                        # Updates average distance\n",
    "                        if (COM_average_distance != 0):\n",
    "                            COM_average_distance = COM_average_distance * (COM_total_tweets[date] - 1) / COM_total_tweets[date] + distance / COM_total_tweets[date]\n",
    "                        else:\n",
    "                            COM_average_distance = distance\n",
    "                            \n",
    "                        print('COM average distance:', COM_average_distance, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "                        # !! TWEET PARSING !!\n",
    "\n",
    "                        # If the team is playing\n",
    "                        \n",
    "                        \n",
    "                        # TODO: UNCOMMENT!\n",
    "                        # if date in dates and hour >= 10 and lang == 'en':\n",
    "\n",
    "                        # Increment total number of tweets\n",
    "                        increment_dict(date, IP_total_tweets)\n",
    "                        increment_dict(hour, IP_tweets_per_hour)\n",
    "\n",
    "                        # Increment total BoW\n",
    "                        for word in words:\n",
    "                            increment_dict(word, IP_total_bow)\n",
    "\n",
    "                        # Updates average distance\n",
    "                        if (IP_average_distance != 0):\n",
    "                            IP_average_distance = IP_average_distance * (IP_total_tweets[date] - 1) / IP_total_tweets[date] + distance / IP_total_tweets[date]\n",
    "                        else:\n",
    "                            IP_average_distance = distance\n",
    "\n",
    "\n",
    "                        # !! POLARITY !!\n",
    "\n",
    "                        scores = sia.polarity_scores(text)  # Create a dict of scores\n",
    "\n",
    "                        # Loop the dict of scores and increment the dicts if necessary\n",
    "                        for key in scores:\n",
    "                            if key == 'neg':\n",
    "                                if scores[key] == 0.0:\n",
    "                                    increment_dict(date, IP_neg_polarity)\n",
    "                            if key == 'neu':\n",
    "                                if scores[key] == 0.0:\n",
    "                                    increment_dict(date, IP_neu_polarity)\n",
    "                            if key == 'pos':\n",
    "                                if scores[key] == 0.0:\n",
    "                                    increment_dict(date, IP_pos_polarity)\n",
    "                            if key == 'compound':\n",
    "                                if scores[key] == 0.0:\n",
    "                                    increment_dict(date, IP_com_polarity)\n",
    "\n",
    "\n",
    "                        # !! COGNITION !!\n",
    "\n",
    "                        tokens = nltk.word_tokenize(text)  # Create a list of tokens\n",
    "                        tokens_tagged = nltk.pos_tag(tokens, tagset='universal')\n",
    "\n",
    "                        pos_tokens = ''\n",
    "\n",
    "                        # Loop the dict of tokens\n",
    "                        for token in tokens_tagged:\n",
    "                            pos_tokens += token[1] + ' '\n",
    "\n",
    "                        sentence = str(pos_tokens)  # 'Whoever is happy will make others happy too'\n",
    "                        trigrams = ngrams(sentence.split(), 3)\n",
    "                        quadgrams = ngrams(sentence.split(), 4)\n",
    "\n",
    "                        trigram_list = []\n",
    "                        for item in trigrams:\n",
    "                            trigram_list.append(item[0])\n",
    "                            trigram_list.append(item[1])\n",
    "                            trigram_list.append(item[2])\n",
    "\n",
    "                        quadgram_list = []\n",
    "                        for item in quadgrams:\n",
    "                            quadgram_list.append(item[0])\n",
    "                            quadgram_list.append(item[1])\n",
    "                            quadgram_list.append(item[2])\n",
    "                            quadgram_list.append(item[3])\n",
    "\n",
    "                        # If in old list, add to dict\n",
    "                        if trigram_list in super_list_old or quadgram_list in super_list_old:\n",
    "                            increment_dict(date, IP_yes_cognition_old)\n",
    "                        else:\n",
    "                            increment_dict(date, IP_no_cognition_old)\n",
    "\n",
    "                        # If in new list, add to dict\n",
    "                        if trigram_list in super_list_new or quadgram_list in super_list_new:\n",
    "                            increment_dict(date, IP_yes_cognition_new)\n",
    "                        else:\n",
    "                            increment_dict(date, IP_no_cognition_new)\n",
    "\n",
    "\n",
    "                        # !! SENTIMENT !!\n",
    "\n",
    "                        # If in old happy emoticons, add to dict\n",
    "                        for emoticon in emoticons_happy_old:\n",
    "                            if text.lower().find(emoticon) != -1:\n",
    "                                increment_dict(date, IP_happy_sentiment_old)\n",
    "                                increment_dict(date, IP_general_sentiment_old)\n",
    "                                break;\n",
    "\n",
    "                        # If in old sad emoticons, add to dict\n",
    "                        for emoticon in emoticons_sad_old:\n",
    "                            if text.lower().find(emoticon) != -1:\n",
    "                                increment_dict(date, IP_sad_sentiment_old)\n",
    "                                increment_dict(date, IP_general_sentiment_old)\n",
    "                                break;\n",
    "\n",
    "                        # If in new happy emoticons, add to dict\n",
    "                        for emoticon in emoticons_happy_new:\n",
    "                            if text.lower().find(emoticon) != -1:\n",
    "                                increment_dict(date, IP_happy_sentiment_new)\n",
    "                                increment_dict(date, IP_general_sentiment_new)\n",
    "                                break;\n",
    "\n",
    "                        # If in new sad emoticons, add to dict\n",
    "                        for emoticon in emoticons_sad_new:\n",
    "                            if text.lower().find(emoticon) != -1:\n",
    "                                increment_dict(date, IP_sad_sentiment_new)\n",
    "                                increment_dict(date, IP_general_sentiment_new)\n",
    "                                break;\n",
    "\n",
    "                        # TODO: Delete me\n",
    "                        end_parse_time = time.time()\n",
    "                        print('\\nTWEET PARSING TIME  - ', end_parse_time - start_parse_time, '\\n\\n')\n",
    "                        \n",
    "                        time.sleep(1)\n",
    "\n",
    "\n",
    "\n",
    "                        # If the team is NOT playing\n",
    "                        # TODO: UNCOMMENT\n",
    "#                         else:\n",
    "\n",
    "#                             # Increment total number of tweets\n",
    "#                             increment_dict(date, NP_total_tweets)\n",
    "#                             increment_dict(hour, NP_tweets_per_hour)\n",
    "\n",
    "#                             # Increment total BoW\n",
    "#                             for word in words:\n",
    "#                                 increment_dict(word, NP_total_bow)\n",
    "\n",
    "#                             # Updates average distance\n",
    "#                             if (IP_average_distance != 0):\n",
    "#                                 IP_average_distance = IP_average_distance * (IP_total_tweets[date] - 1) / IP_total_tweets[date] + distance / IP_total_tweets[date]\n",
    "#                             else:\n",
    "#                                 IP_average_distance = distance\n",
    "\n",
    "\n",
    "#                             # !! POLARITY !!\n",
    "\n",
    "#                             scores = sia.polarity_scores(text)  # Create a dict of scores\n",
    "\n",
    "#                             # Loop the dict of scores and increment the dicts if necessary\n",
    "#                             for key in scores:\n",
    "#                                 if key == 'neg':\n",
    "#                                     if scores[key] == 0.0:\n",
    "#                                         increment_dict(date, NP_neg_polarity)\n",
    "#                                 if key == 'neu':\n",
    "#                                     if scores[key] == 0.0:\n",
    "#                                         increment_dict(date, NP_neu_polarity)\n",
    "#                                 if key == 'pos':\n",
    "#                                     if scores[key] == 0.0:\n",
    "#                                         increment_dict(date, NP_pos_polarity)\n",
    "#                                 if key == 'compound':\n",
    "#                                     if scores[key] == 0.0:\n",
    "#                                         increment_dict(date, NP_com_polarity)\n",
    "\n",
    "\n",
    "#                             # !! COGNITION !!\n",
    "\n",
    "#                             tokens = nltk.word_tokenize(text)  # Create a list of tokens\n",
    "#                             tokens_tagged = nltk.pos_tag(tokens, tagset='universal')\n",
    "\n",
    "#                             pos_tokens = ''\n",
    "\n",
    "#                             # Loop the dict of tokens\n",
    "#                             for token in tokens_tagged:\n",
    "#                                 pos_tokens += token[1] + ' '\n",
    "\n",
    "#                             sentence = str(pos_tokens)  # 'Whoever is happy will make others happy too'\n",
    "#                             trigrams = ngrams(sentence.split(), 3)\n",
    "#                             quadgrams = ngrams(sentence.split(), 4)\n",
    "\n",
    "#                             trigram_list = []\n",
    "#                             for item in trigrams:\n",
    "#                                 trigram_list.append(item[0])\n",
    "#                                 trigram_list.append(item[1])\n",
    "#                                 trigram_list.append(item[2])\n",
    "\n",
    "#                             quadgram_list = []\n",
    "#                             for item in quadgrams:\n",
    "#                                 quadgram_list.append(item[0])\n",
    "#                                 quadgram_list.append(item[1])\n",
    "#                                 quadgram_list.append(item[2])\n",
    "#                                 quadgram_list.append(item[3])\n",
    "\n",
    "#                             # If in old list, add to dict\n",
    "#                             if trigram_list in super_list_old or quadgram_list in super_list_old:\n",
    "#                                 increment_dict(date, NP_yes_cognition_old)\n",
    "#                             else:\n",
    "#                                 increment_dict(date, NP_no_cognition_old)\n",
    "\n",
    "#                             # If in new list, add to dict\n",
    "#                             if trigram_list in super_list_new or quadgram_list in super_list_new:\n",
    "#                                 increment_dict(date, NP_yes_cognition_new)\n",
    "#                             else:\n",
    "#                                 increment_dict(date, NP_no_cognition_new)\n",
    "\n",
    "\n",
    "#                             # !! SENTIMENT !!\n",
    "\n",
    "#                             # If in old happy emoticons, add to dict\n",
    "#                             for emoticon in emoticons_happy_old:\n",
    "#                                 if text.lower().find(emoticon) != -1:\n",
    "#                                     increment_dict(date, NP_happy_sentiment_old)\n",
    "#                                     increment_dict(date, NP_general_sentiment_old)\n",
    "#                                     break;\n",
    "\n",
    "#                             # If in old sad emoticons, add to dict\n",
    "#                             for emoticon in emoticons_sad_old:\n",
    "#                                 if text.lower().find(emoticon) != -1:\n",
    "#                                     increment_dict(date, NP_sad_sentiment_old)\n",
    "#                                     increment_dict(date, NP_general_sentiment_old)\n",
    "#                                     break;\n",
    "\n",
    "#                             # If in new happy emoticons, add to dict\n",
    "#                             for emoticon in emoticons_happy_new:\n",
    "#                                 if text.lower().find(emoticon) != -1:\n",
    "#                                     increment_dict(date, NP_happy_sentiment_new)\n",
    "#                                     increment_dict(date, NP_general_sentiment_new)\n",
    "#                                     break;\n",
    "\n",
    "#                             # If in new sad emoticons, add to dict\n",
    "#                             for emoticon in emoticons_sad_new:\n",
    "#                                 if text.lower().find(emoticon) != -1:\n",
    "#                                     increment_dict(date, NP_sad_sentiment_new)\n",
    "#                                     increment_dict(date, NP_general_sentiment_new)\n",
    "#                                     break;\n",
    "\n",
    "#                             # TODO: Delete me\n",
    "#                             end_parse_time = time.time()\n",
    "#                             print('\\nTWEET PARSING TIME  - ', end_parse_time - start_parse_time, '\\n')\n",
    "\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "    # Write to combined average distance + total BoW/tweet text files\n",
    "    with open(team_abbrev + '_COM_average_distance.txt', 'w') as new_file:\n",
    "        new_file.write(COM_average_distance)\n",
    "    write_dict_to_file(team_abbrev + '_COM_total_bow.txt', COM_total_bow)\n",
    "    write_dict_to_file(team_abbrev + '_COM_total_tweets.txt', COM_total_tweets)\n",
    "    write_dict_to_file(team_abbrev + '_COM_tweets_per_hour.txt', COM_tweets_per_hour)\n",
    "\n",
    "\n",
    "    # TEAM IS PLAYING\n",
    "\n",
    "    # Write to average distance + total BoW/tweet text files\n",
    "    with open(team_abbrev + '_IP_average_distance.txt', 'w') as new_file:\n",
    "        new_file.write(IP_average_distance)\n",
    "    write_dict_to_file(team_abbrev + '_IP_total_bow.txt', total_bow)\n",
    "    write_dict_to_file(team_abbrev + '_IP_total_tweets.txt', total_tweets)\n",
    "    write_dict_to_file(team_abbrev + '_IP_tweets_per_hour.txt', IP_tweets_per_hour)\n",
    "\n",
    "    # Write to polarity text files\n",
    "    write_dict_to_file(team_abbrev + '_IP_negative.txt', neg_polarity)\n",
    "    write_dict_to_file(team_abbrev + '_IP_neutral.txt', neu_polarity)\n",
    "    write_dict_to_file(team_abbrev + '_IP_positive.txt', pos_polarity)\n",
    "    write_dict_to_file(team_abbrev + '_IP_compound.txt', com_polarity)\n",
    "\n",
    "    # Write to cognition text files old\n",
    "    write_dict_to_file(team_abbrev + '_IP_yes_old.txt', yes_cognition_old)\n",
    "    write_dict_to_file(team_abbrev + '_IP_no_old.txt', no_cognition_old)\n",
    "\n",
    "    # Write to cognition text files new\n",
    "    write_dict_to_file(team_abbrev + '_IP_yes_new.txt', yes_cognition_new)\n",
    "    write_dict_to_file(team_abbrev + '_IP_no_new.txt', no_cognition_new)\n",
    "\n",
    "    # Write to sentiment text files old\n",
    "    write_dict_to_file(team_abbrev + '_IP_happy_old.txt', happy_sentiment_old)\n",
    "    write_dict_to_file(team_abbrev + '_IP_sad_old.txt', sad_sentiment_old)\n",
    "    write_dict_to_file(team_abbrev + '_IP_general_old.txt', general_sentiment_old)\n",
    "\n",
    "    # Write to sentiment text files new\n",
    "    write_dict_to_file(team_abbrev + '_IP_happy_new.txt', happy_sentiment_new)\n",
    "    write_dict_to_file(team_abbrev + '_IP_sad_new.txt', sad_sentiment_new)\n",
    "    write_dict_to_file(team_abbrev + '_IP_general_new.txt', general_sentiment_new)\n",
    "\n",
    "\n",
    "    # TEAM NOT PLAYING:\n",
    "\n",
    "    # Write to average distance text file\n",
    "    with open(team_abbrev + '_NP_average_distance.txt', 'w') as new_file:\n",
    "        new_file.write(NP_average_distance)\n",
    "\n",
    "    # Write to total BoW/tweet text files\n",
    "    write_dict_to_file(team_abbrev + '_NP_total_bow.txt', NP_total_bow)\n",
    "    write_dict_to_file(team_abbrev + '_NP_total_tweets.txt', NP_total_tweets)\n",
    "    write_dict_to_file(team_abbrev + '_NP_tweets_per_hour.txt', NP_tweets_per_hour)\n",
    "\n",
    "    # Write to polarity text files\n",
    "    write_dict_to_file(team_abbrev + '_NP_negative.txt', NP_neg_polarity)\n",
    "    write_dict_to_file(team_abbrev + '_NP_neutral.txt', NP_neu_polarity)\n",
    "    write_dict_to_file(team_abbrev + '_NP_positive.txt', NP_pos_polarity)\n",
    "    write_dict_to_file(team_abbrev + '_NP_compound.txt', NP_com_polarity)\n",
    "\n",
    "    # Write to cognition text files old\n",
    "    write_dict_to_file(team_abbrev + '_NP_yes_old.txt', NP_yes_cognition_old)\n",
    "    write_dict_to_file(team_abbrev + '_NP_no_old.txt', NP_no_cognition_old)\n",
    "\n",
    "    # Write to cognition text files new\n",
    "    write_dict_to_file(team_abbrev + '_NP_yes_new.txt', NP_yes_cognition_new)\n",
    "    write_dict_to_file(team_abbrev + '_NP_no_new.txt', NP_no_cognition_new)\n",
    "\n",
    "    # Write to sentiment text files old\n",
    "    write_dict_to_file(team_abbrev + '_NP_happy_old.txt', NP_happy_sentiment_old)\n",
    "    write_dict_to_file(team_abbrev + '_NP_sad_old.txt', NP_sad_sentiment_old)\n",
    "    write_dict_to_file(team_abbrev + '_NP_general_old.txt', NP_general_sentiment_old)\n",
    "\n",
    "    # Write to sentiment text files new\n",
    "    write_dict_to_file(team_abbrev + '_NP_happy_new.txt', NP_happy_sentiment_new)\n",
    "    write_dict_to_file(team_abbrev + '_NP_sad_new.txt', NP_sad_sentiment_new)\n",
    "    write_dict_to_file(team_abbrev + '_NP_general_new.txt', NP_general_sentiment_new)\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    print(team_name, 'DONE: Runtime  - ', display_time(t1 - t0))\n",
    "\n",
    "\n",
    "# ALL TEAMS\n",
    "\n",
    "# Write to total BoW/tweet text files\n",
    "write_dict_to_file('ALL_TEAMS_total_bow.txt', ALL_TEAMS_total_bow)\n",
    "write_dict_to_file('ALL_TEAMS_total_tweets.txt', ALL_TEAMS_total_tweets)\n",
    "\n",
    "\n",
    "t_final = time.time()\n",
    "\n",
    "print('\\nTOTAL RUNTIME  - ', display_time(t_final - t0), '\\n')\n",
    "print('El Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862eecb-65cf-4216-8233-c3a756dc8025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34223e88-33e4-4665-be26-3c3274227778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c4d1dc-0893-4e33-8a5e-4c6fd78a5d64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7f5e1-d830-4b88-939a-35c2fad895f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8434dad-a9df-4f66-8975-eb62a0cffae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c840c8-c4a1-4957-9fd8-d6a0cd28426e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f2721-0de5-43ba-9978-c7e340adcdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e9895-a698-4f81-83ca-6040ac85a1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68824b5d-e21d-47bc-9f4a-47404de53a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625109bf-9876-4ec3-8256-d8682142570d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a6af6-b2c7-459d-9c63-75ead77cb5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784a219-a51c-4ee6-90eb-f66aa5f83076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432106db-0bed-447a-a043-2a7c32d2d740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6fdd5da-ab50-40ad-8a87-c6509846fe33",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m blob \u001b[38;5;241m=\u001b[39m BUCKET_NAME_TWO\u001b[38;5;241m.\u001b[39mblob(status_log_bucket_name\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m             tweet_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/cloud/storage/fileio.py:165\u001b[0m, in \u001b[0;36mBlobReader.read1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread1\u001b[39m(\u001b[38;5;28mself\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/cloud/storage/fileio.py:142\u001b[0m, in \u001b[0;36mBlobReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Download the blob. Checksumming must be disabled as we are using\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# chunked downloads, and the server only knows the checksum of the\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# entire file.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_as_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RequestRangeNotSatisfiable:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;66;03m# We've reached the end of the file. Python file objects should\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# return an empty response in this case, not raise an error.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/cloud/storage/blob.py:1401\u001b[0m, in \u001b[0;36mBlob.download_as_bytes\u001b[0;34m(self, client, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1399\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_require_client(client)\n\u001b[1;32m   1400\u001b[0m string_buffer \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m-> 1401\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_blob_to_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1402\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstring_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_etag_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_etag_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_etag_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_etag_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_generation_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_generation_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_generation_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_metageneration_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_metageneration_not_match\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1416\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m string_buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/cloud/storage/client.py:1151\u001b[0m, in \u001b[0;36mClient.download_blob_to_file\u001b[0;34m(self, blob_or_uri, file_obj, start, end, raw_download, if_etag_match, if_etag_not_match, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   1149\u001b[0m transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1151\u001b[0m     \u001b[43mblob_or_uri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchecksum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchecksum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m resumable_media\u001b[38;5;241m.\u001b[39mInvalidResponse \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1164\u001b[0m     _raise_from_invalid_response(exc)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/cloud/storage/blob.py:989\u001b[0m, in \u001b[0;36mBlob._do_download\u001b[0;34m(self, transport, file_obj, download_url, headers, start, end, raw_download, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m    980\u001b[0m     download \u001b[38;5;241m=\u001b[39m klass(\n\u001b[1;32m    981\u001b[0m         download_url,\n\u001b[1;32m    982\u001b[0m         stream\u001b[38;5;241m=\u001b[39mfile_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    986\u001b[0m         checksum\u001b[38;5;241m=\u001b[39mchecksum,\n\u001b[1;32m    987\u001b[0m     )\n\u001b[1;32m    988\u001b[0m     download\u001b[38;5;241m.\u001b[39m_retry_strategy \u001b[38;5;241m=\u001b[39m retry_strategy\n\u001b[0;32m--> 989\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mdownload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconsume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_headers_from_download(response)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/resumable_media/requests/download.py:232\u001b[0m, in \u001b[0;36mDownload.consume\u001b[0;34m(self, transport, timeout)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_to_stream(result)\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_request_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretriable_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_status_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_strategy\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/resumable_media/requests/_request_helpers.py:148\u001b[0m, in \u001b[0;36mwait_and_retry\u001b[0;34m(func, get_status_code, retry_strategy)\u001b[0m\n\u001b[1;32m    146\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _CONNECTION_ERROR_CLASSES \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m     error \u001b[38;5;241m=\u001b[39m e  \u001b[38;5;66;03m# Fall through to retry, if there are retries left.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/resumable_media/requests/download.py:205\u001b[0m, in \u001b[0;36mDownload.consume.<locals>.retriable_request\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m         query_param \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object_generation}\n\u001b[1;32m    203\u001b[0m         url \u001b[38;5;241m=\u001b[39m _helpers\u001b[38;5;241m.\u001b[39madd_query_parameters(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmedia_url, query_param)\n\u001b[0;32m--> 205\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# If a generation hasn't been specified, and this is the first response we get, let's record the\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# generation. In future requests we'll specify the generation query param to avoid data races.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object_generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/google/auth/transport/requests.py:549\u001b[0m, in \u001b[0;36mAuthorizedSession.request\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TimeoutGuard(remaining_time) \u001b[38;5;28;01mas\u001b[39;00m guard:\n\u001b[0;32m--> 549\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAuthorizedSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m remaining_time \u001b[38;5;241m=\u001b[39m guard\u001b[38;5;241m.\u001b[39mremaining_timeout\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# If the response indicated that the credentials needed to be\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# refreshed, then refresh the credentials and re-attempt the\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# request.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# A stored token may expire between the time it is retrieved and\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# the time the request is made, so we may need to try twice.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/requests/sessions.py:542\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    537\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m: timeout,\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m'\u001b[39m: allow_redirects,\n\u001b[1;32m    540\u001b[0m }\n\u001b[1;32m    541\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 542\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/requests/sessions.py:655\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    654\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 655\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    658\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/requests/adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[0;32m--> 439\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/urllib3/connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    698\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 699\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    713\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/urllib3/connectionpool.py:445\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    440\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/site-packages/urllib3/connectionpool.py:440\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    445\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/http/client.py:1371\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/http/client.py:319\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/http/client.py:280\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 280\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "\n",
    "    # remove old style retweet text 'RT'\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "\n",
    "    # remove hashtags\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    \n",
    "    return tweet\n",
    "\n",
    "intervals = (\n",
    "    ('weeks', 604800),  # 60 * 60 * 24 * 7\n",
    "    ('days', 86400),    # 60 * 60 * 24\n",
    "    ('hours', 3600),    # 60 * 60\n",
    "    ('minutes', 60),\n",
    "    ('seconds', 1),\n",
    ")\n",
    "\n",
    "def display_time(seconds, granularity=2):\n",
    "    result = []\n",
    "\n",
    "    for name, count in intervals:\n",
    "        value = seconds // count\n",
    "        if value:\n",
    "            seconds -= value * count\n",
    "            if value == 1:\n",
    "                name = name.rstrip('s')\n",
    "            result.append(\"{} {}\".format(round(value), name))\n",
    "    return ', '.join(result[:granularity])\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "\n",
    "for team_name in team_names:\n",
    "    \n",
    "    dates = []\n",
    "    \n",
    "    team_abbrev = team_abbrevs[team_names.index(team_name)]\n",
    "\n",
    "    for i in range(16, 20):\n",
    "        with open(('Data/Team_WL/' + str(team_abbrev) + str(i) + '.csv'), newline='', encoding='utf-8-sig') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "            for row in reader:\n",
    "                date = row[0]\n",
    "                if (date.startswith('d')):\n",
    "                    continue\n",
    "                if not date.startswith('1'):\n",
    "                    date = '0' + date\n",
    "                if not date[5] == '/':\n",
    "                    date = date[:3] + '0' + date[3:]\n",
    "                date = date[6:] + '-' + date[:2] + '-' + date[3:-5]\n",
    "                dates.append(date)\n",
    "\n",
    "    # Dictionaries: Key - Date    Value - Number of tweets\n",
    "    neg_polarity_dict = {}\n",
    "    neu_polarity_dict = {}\n",
    "    pos_polarity_dict = {}\n",
    "    com_polarity_dict = {}\n",
    "    \n",
    "    yes_cognition_dict = {}\n",
    "    no_cognition_dict = {}\n",
    "    \n",
    "    happy_sentiment_dict = {}\n",
    "    sad_sentiment_dict = {}\n",
    "    general_sentiment_dict = {}\n",
    " \n",
    "    object_generator = storage_client.list_blobs(BUCKET_NAME, prefix=team_name, delimiter=None)\n",
    "\n",
    "    for status_log_bucket_name in object_generator:\n",
    "        client2 = storage.Client()\n",
    "        BUCKET_NAME_TWO = client2.bucket(BUCKET_NAME)\n",
    "        blob = BUCKET_NAME_TWO.blob(status_log_bucket_name.name)\n",
    "\n",
    "        with blob.open('rt') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    tweet_dict = json.loads(line)\n",
    "\n",
    "                    tweet = Tweet(tweet_dict)\n",
    "\n",
    "                    text = clean_tweets(tweet.all_text)\n",
    "\n",
    "                    # Extracts date and verifies its a day the Sox play\n",
    "                    date = tweet.created_at_string[0:10]\n",
    "\n",
    "                    if date in dates:\n",
    "                        if date[:4] == '2019':\n",
    "\n",
    "                            # POLARITY\n",
    "\n",
    "                            scores = sia.polarity_scores(text) # Create a dict of scores\n",
    "\n",
    "                            # Loop the dict of scores\n",
    "                            for key in scores:\n",
    "                                if key == 'neg':\n",
    "                                    if scores[key] == 0.0:\n",
    "                                        if date in neg_polarity_dict.keys():\n",
    "                                            neg_polarity_dict[date] += 1\n",
    "                                        else:\n",
    "                                            neg_polarity_dict[date] = 1\n",
    "                                if key == 'neu':\n",
    "                                    if scores[key] == 0.0:\n",
    "                                        if date in neu_polarity_dict.keys():\n",
    "                                            neu_polarity_dict[date] += 1\n",
    "                                        else:\n",
    "                                            neu_polarity_dict[date] = 1\n",
    "                                if key == 'pos':\n",
    "                                    if scores[key] == 0.0:\n",
    "                                        if date in pos_polarity_dict.keys():\n",
    "                                            pos_polarity_dict[date] += 1\n",
    "                                        else:\n",
    "                                            pos_polarity_dict[date] = 1\n",
    "                                if key == 'compound':\n",
    "                                    if scores[key] == 0.0:\n",
    "                                        if date in com_polarity_dict.keys():\n",
    "                                            com_polarity_dict[date] += 1\n",
    "                                        else:\n",
    "                                            com_polarity_dict[date] = 1\n",
    "\n",
    "\n",
    "                            # COGNITION\n",
    "\n",
    "                            tokens = nltk.word_tokenize(text) # Create a list of tokens\n",
    "                            tokens_tagged = nltk.pos_tag(tokens, tagset='universal')\n",
    "\n",
    "                            pos_tokens = ''\n",
    "\n",
    "                            # Loop the dict of tokens\n",
    "                            for token in tokens_tagged:\n",
    "                                pos_tokens += token[1] + ' '\n",
    "\n",
    "                            sentence = str(pos_tokens)#'Whoever is happy will make others happy too'\n",
    "                            trigrams = ngrams(sentence.split(), 3)\n",
    "                            quadgrams = ngrams(sentence.split(), 4)\n",
    "\n",
    "                            trigram_list = []\n",
    "                            for item in trigrams:\n",
    "                                trigram_list.append(item[0])\n",
    "                                trigram_list.append(item[1])\n",
    "                                trigram_list.append(item[2])\n",
    "\n",
    "                            quadgram_list = []\n",
    "                            for item in quadgrams:\n",
    "                                quadgram_list.append(item[0])\n",
    "                                quadgram_list.append(item[1])\n",
    "                                quadgram_list.append(item[2])\n",
    "                                quadgram_list.append(item[3])\n",
    "\n",
    "                            if trigram_list in super_list or quadgram_list in super_list:\n",
    "                                if date in yes_cognition_dict.keys():\n",
    "                                    yes_cognition_dict[date] += 1\n",
    "                                else:\n",
    "                                    yes_cognition_dict[date] = 1\n",
    "                            else:\n",
    "                                if date in no_cognition_dict.keys():\n",
    "                                    no_cognition_dict[date] += 1\n",
    "                                else:\n",
    "                                    no_cognition_dict[date] = 1\n",
    "\n",
    "\n",
    "                            # SENTIMENT        \n",
    "\n",
    "                            for emoticon in emoticons_happy:\n",
    "                                if text.lower().find(emoticon) != -1:\n",
    "                                  # There's a happy emoticon: Adds to the dict\n",
    "                                  if date in happy_sentiment_dict.keys():\n",
    "                                    happy_sentiment_dict[date] += 1\n",
    "                                  else:\n",
    "                                    happy_sentiment_dict[date] = 1\n",
    "                                  if date in general_sentiment_dict.keys():\n",
    "                                    general_sentiment_dict[date] += 1\n",
    "                                  else:\n",
    "                                    general_sentiment_dict[date] = 1\n",
    "                                  break;\n",
    "\n",
    "                            for emoticon in emoticons_sad:\n",
    "                                if text.lower().find(emoticon) != -1:\n",
    "                                  # There's a sad emoticon: Adds to the dict\n",
    "                                  if date in sad_sentiment_dict.keys():\n",
    "                                    sad_sentiment_dict[date] += 1\n",
    "                                  else:\n",
    "                                    sad_sentiment_dict[date] = 1\n",
    "                                  if date in general_sentiment_dict.keys():\n",
    "                                    general_sentiment_dict[date] += 1\n",
    "                                  else:\n",
    "                                    general_sentiment_dict[date] = 1\n",
    "                                  break;\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    \n",
    "#     # Write to polarity text files\n",
    "#     with open(team_abbrev + '_negative.txt', 'w') as new_file: \n",
    "#         for key, value in neg_polarity_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "#     with open(team_abbrev + '_neutral.txt', 'w') as new_file: \n",
    "#         for key, value in neu_polarity_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "#     with open(team_abbrev + '_positive.txt', 'w') as new_file: \n",
    "#         for key, value in pos_polarity_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "#     with open(team_abbrev + '_compound.txt', 'w') as new_file: \n",
    "#         for key, value in com_polarity_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "            \n",
    "            \n",
    "#     # Write to cognition text files\n",
    "#     with open(team_abbrev + '_yes.txt', 'w') as new_file: \n",
    "#         for key, value in yes_cognition_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "#     with open(team_abbrev + '_no.txt', 'w') as new_file: \n",
    "#         for key, value in no_cognition_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "            \n",
    "            \n",
    "#     # Write to sentiment text files\n",
    "#     with open(team_abbrev + '_happy.txt', 'w') as new_file: \n",
    "#         for key, value in happy_sentiment_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "#     with open(team_abbrev + '_sad.txt', 'w') as new_file: \n",
    "#         for key, value in sad_sentiment_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "#     with open(team_abbrev + '_general.txt', 'w') as new_file: \n",
    "#         for key, value in general_sentiment_dict.items(): \n",
    "#             new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    # Write to polarity text files\n",
    "    with open(team_abbrev + '19_negative.txt', 'w') as new_file: \n",
    "        for key, value in neg_polarity_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    with open(team_abbrev + '19_neutral.txt', 'w') as new_file: \n",
    "        for key, value in neu_polarity_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    with open(team_abbrev + '19_positive.txt', 'w') as new_file: \n",
    "        for key, value in pos_polarity_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    with open(team_abbrev + '19_compound.txt', 'w') as new_file: \n",
    "        for key, value in com_polarity_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "            \n",
    "            \n",
    "    # Write to cognition text files\n",
    "    with open(team_abbrev + '19_yes.txt', 'w') as new_file: \n",
    "        for key, value in yes_cognition_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    with open(team_abbrev + '19_no.txt', 'w') as new_file: \n",
    "        for key, value in no_cognition_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "            \n",
    "            \n",
    "    # Write to sentiment text files\n",
    "    with open(team_abbrev + '19_happy.txt', 'w') as new_file: \n",
    "        for key, value in happy_sentiment_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    with open(team_abbrev + '19_sad.txt', 'w') as new_file: \n",
    "        for key, value in sad_sentiment_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "\n",
    "    with open(team_abbrev + '19_general.txt', 'w') as new_file: \n",
    "        for key, value in general_sentiment_dict.items(): \n",
    "            new_file.write('%s : %d\\n' % (key, value))\n",
    "            \n",
    "    t1 = time.time()\n",
    "            \n",
    "    print(team_name, 'runtime  - ', display_time(t1-t0))\n",
    "                    \n",
    "\n",
    "t_final = time.time()\n",
    "        \n",
    "print('\\nTOTAL RUNTIME  - ', display_time(t_final-t0), '\\n')\n",
    "print('El Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfedbb85-f03c-4ebd-87f6-370c7bbc9706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
